\section{Методы}
\subsection{Методы кодировки последовательности}
Все нуклеотидные последовательности были закодированны в one-hot-encoded векторы (единичные нуклеотиды) и матрицы (последовательности), где каждому нуклеотиду соответствует один из четырехмерных векторов (0, 0, 0, 1), (0, 0, 1, 0), (0, 1, 0, 0), (1, 0, 0, 0). Это позволяет добиться независимого влияния нуклеотидов на предсказание и используется в категориальных предсказаниях.

\subsection{Использованные функции}

В качестве функции активации использовавалась функция softmax и relu (уравнение \ref{eq:softmax}).
\begin{align} \label{eq:softmax}
softmax(x) &= exp(x - \max_{axis}(x)) \\
relu(x) &= max(x, 0)
\end{align}

Функция потерь во всех архитектурах -- категориалная кроссэнтропия (уравнение \ref{eq:cross}).
\begin{equation} \label{eq:cross}
categorical\_crossentropy(y_{pred}, y_{true}) = -\sum_{x}{y_{pred}(x)\log(y_{true}(x))}
\end{equation}

Для статистического сравнения выборок использовался критерий Манна-Уитни.

\subsection{Простейшие нейросетевые модели}
\subsubsection{Построение выборки контекстов}

Последовательности выбирались из генома  Escherichia coli (Escherichia coli str. K-12 substr. MG1655, сборка GCF 000005845.2).

Выборка представляет собой набор контекстов (предикторных областей) определенной длины и соответственных нуклеотидов для предсказания.

Выборка состояла из нескольких частей --  для обучения алгоритма (train), валидации в процессе обучения (validate), тестирования (test). При этом эти части были взяты из непересекающихся геномных областей, чтобы предотвратить выучивание алгоритмом последовательности нуклеотидов.

Для статистической проверки каждого метода было построено 30 выборок. Во всех 30 выборках области, соответствующие тренировочной и валидационной части, были разные.

Размер тренировочной выборки обычно составлял 100,000 нуклеотидов, валидационной и тренирочной -- по одной десятой, соответственно 10,000 и 10,000.

Предикторные области (контексты) для каждого нуклеотида находились с 5' конца нуклеотида, их размер варьировал -- 3, 6, 12, 24 нуклеотида. Также создавались выборки со предикторной областью, сдвинутой относительно предсказываемого нуклеотида в 5' сторону на 1, 2, 3, 6, 12, 50 нуклеотидов.

Все предикторные обасти и предсказываемые нуклеотиды были закодированы в виде one-hot-encoded векторов и матриц.

\subsubsection{Архитектура нейронных сетей}
Все нейронные сети были реализованы с помощью библиотек Keras\cite{chollet_keras_2015} , TensorFlow.
Для работы использовалось несколько архитектур и типов слоев.

\input{tables/all_schemes}

{\bfseries Полносвязные модели} (DNN model 1 - рисунок \ref{fig:dnn_1_scheme}). Первая простейшая полносвязная модель состоит из входного слоя, принимающего нуклеотидный контекст, слоя, уплощающего данные в один вектор, четырех полносвязных нейронов с функцией активации softmax, выходом которых являются вероятности для четырех выходных букв. Число параметров модели $16n + 4$, где $n$ -- размер контекста.

(DNN model 2 -- рисунок \ref{fig:dnn_2_scheme}). В полносвязную модель добавлен второй слой нейронов. Число параметров модели $16n + 24$, где $n$ -- размер контекста.


{\bfseries Сверточные модели} (CNN model 1 - рисунок \ref{fig:cnn_schemes}).
Простейшая сверточная модель состоит из слоя сверточных нейронов (с функцией активации relu), который работает непосредственно с матрицей контекста, далее выход свертки уплощается в вектор и подается в слой из 4 решающих выходных полносвязных нейронов (с функцией активации softmax). Конфигурация сверточного слоя может быть различной. Мы исследовали комбинации из разного числа нейронов, с разным размером фильтра (kernel), которые обрабатывают контекст с различным шагом (stride, по умолчанию шаг сверточного фильтра равен 0).

Были использованы следующие конфигурации сверточного слоя: \begin{enumerate}
		\item (kernel = 3)*3, три сверточных фильтра размером 3
		\item (kernel = 6)*3, три сверточных фильтра размером 6
		\item (kernel = 3)*3 stride = 3, три сверточных фильтра размером 3, которые обрабатывают матрицу с шагом 3.
	\end{enumerate}
 
{\bfseries Рекуррентыне модели } (RNN model 1 - рисунок \ref{fig:rnn_scheme}) Рекуррентная модель состояла из одного LSTM (long short-term memory) слоя, который содержал разное число скрытых состояний, и выходного слоя полносвязных нейронов с функцией активации softmax. 


\subsubsection{Процесс обучения}
Все архитектуры компилировались с использованием оптимизатора Adam \cite{kingma_adam:_2014} с параметрами по умолчанию (learning rate = 0.01). Во время обучения контролировалась точность предсказания на валидационной выборке, с прекращением роста точности обучение останавливалось.


\subsection{Deep Image Prior}
Deep Image Prior --  нейронная сеть с архитектурой автоэнкодера с пробросочными соединениями, подробно описана в \cite{ulyanov_deep_2018}

Данная архитектура была адаптирована для геномной последовательности, закодированной в виде one-hot-encoded матрицы соответственного размера $n\times 4$, где $n$ – длина обрабатываемой геномной области.
В данной архитектуре двумерные функции свертки, пулинга, нормализации, апсемплинга были заменены на соответствующие одномерные аналоги.
Суть подхода закллючается в следующем:

\begin{enumerate}
	\item Выбиралась геномная область длиной 500,000 нуклеотидов. Такой размер области позволял наиболее эффективно проводить расчеты.
	\item На области случайно равномерно выбиралось  10\% нуклеотидов, которые далее предсказывались, которые обозначаются как тест или маска.
	\item Нейронная сеть обучалась получать из случайно сгенерированного массива чисел целевую геномную последовательность. Функция потерь при этом не учитывала тестовые (маскированные) нуклеотиды.
	\item Когда функция потерь достигала низких значений, обучение останавливалось. Проверялось, что же предсказывает модель на месте замаскированных нуклеотидов.
\end{enumerate} 



