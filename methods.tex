\section{Методы}
\subsection{Данные}
Все нуклеотидные последовательности были закодированны в one-hot-encoded векторы (единичные нуклеотиды) и матрицы (последовательности), где каждому нуклеотиду соответствует один из четырехмерных векторов (0, 0, 0, 1), (0, 0, 1, 0), (0, 1, 0, 0), (1, 0, 0, 0). Это позволяет добиться независимого влияния нуклеотидов на предсказание и используется в категориальных предсказаниях.
\subsubsection{Построение выборки контекстов}

Последовательности выбирались из генома  Escherichia coli (Escherichia coli str. K-12 substr. MG1655, сборка GCF 000005845.2).

Выборка представляет собой набор контекстов (предикторных областей) определенной длины и соответственных нуклеотидов для предсказания.

Выборка состояла из нескольких частей --  для обучения алгоритма (train), валидации в процессе обучения (validate), тестирования (test). При этом эти части были взяты из непересекающихся геномных областей, чтобы предотвратить выучивание алгоритмом последовательности нуклеотидов.

Для статистической проверки каждого метода было построено 30 выборок. Во всех 30 выборках области, соответствующие тренировочной и валидационной части, были разные.

Размер тренировочной выборки обычно составлял 100,000 нуклеотидов, валидационной и тренирочной -- по одной десятой, соответственно 10,000 и 10,000.

Предикторные области (контексты) для каждого нуклеотида находились с 5' конца нуклеотида, их размер варьировал -- 3, 6, 12, 24 нуклеотида. Также создавались выборки со предикторной областью, сдвинутой относительно предсказываемого нуклеотида в 5' сторону на 1, 2, 3, 6, 12, 50 нуклеотидов.

В качестве предикторов также использовались контексты с двух сторон от нуклеотида, причем каждый

Все предикторные обасти и предсказываемые нуклеотиды были закодированы в виде one-hot-encoded векторов и матриц.

\subsection{Нейронные сети}

В качестве функции активации использовалась функция softmax и ReLU:
\begin{align} \label{eq:softmax}
\text{softmax}(X) &= exp(X - \max_{axis}(X)) \\
\text{ReLU}(X) &= \max_{x_i \in X}(x_i, 0)
\end{align}
Также внутри рекуррентных слоев используются функции активации:
\begin{align}
	\sigma(X) &= \frac{1}{1+ e^{-X}}  \\
	tanh(X) &= \frac{e^{X} - e^{-X}}{e^{X} + e^{-X}}
\end{align}

В нашем случае функцией потерь служила категориальная кроссэнтропия:
\begin{equation} \label{eq:cross}
\text{categorical\_crossentropy}(Y_{pred}, Y_{true}) = -\sum_{x}{Y_{pred}(X)\log(Y_{true}(X))}
\end{equation}

Во всех архитектурах использовался оптимизатор  Adam \cite{kingma_adam:_2014} с параметрами по умолчанию. 
Во время обучения контролировалась точность предсказания на валидационной выборке, с прекращением роста точности обучение останавливалось.

Для статистического сравнения точности сетей на наборе датасетов использовался критерий Манна-Уитни.

\paragraph{Полносвязный слой}
Каждый из j нейронов в полносвязном слое вычисляет сумму результатов поэлементного произведения вектора входных значений на вектор весов, что можно записать как:
\begin{equation}
	\text{fully_connected}(X)_{j} = X_{1\times L} \cdot W^j_{L\times 1} = x_1 w^j_1 +  \dots x_L w^j_L 
\end{equation}
где $L$ -- длина входного вектора, $X_{1\times L}$ -- входной вектор, $W^j_{L\times 1}$ -- вектор весов.

\paragraph{Сверточный слой}

Для работы с последовательностями была использована одномерная свертка. Слой состоит из нескольких нейронов, каждый со своей весовой матрицей. Каждый нейрон сканирует последовательность вдоль. Под размером нейрона мы понимаем длину скользящего окна. Формально, вычисляется следующее:
\begin{equation}
	\text{convolution}(X)_{oj} = \sum_{k=0}^{K-1}\sum_{c=0}^{C-1} W_{kc}^j X_{i+m, c}
\end{equation}
где $X$ -- это вход, $o$ -- индекс выходной позиции, $j$ -- номер нейрона в слое. Каждое сверточное ядро это матрица $W^j$ размером $K \times C$, где $K$ размер окна, $C$ число входных каналов (в случае первого слоя это 4, для слоев более высокого порядка число каналов определяется число нейронов в предыдущем слое).
\paragraph{Рекуррентные слои} В наших архитектурах мы использовали одну вариацию рекуррентного слоя -- слой долгосрочной памяти (LSTM, long short-term memory)\cite{gers_learning_2006}. Схема показаны на рисунке \ref{fig:lstm}. Нейрон LSTM получает входные данные последовательно в каждый момент времени $\tr{x}{t}$. В каждый момент времени также генерируется выход $\tr{h}{t}$. Для генерации выхода также используются предыдущее скрытое состояние $\tr{h}{t-1}$ и предыдущее значение запоминающего состояния $\tr{c}{t-1}$. Внутри нейрона проводится четыре преобразования с разными активациями ($f_t, i_t, \tilde c_t, o_t$), каждое преобразование имеет две свои матрицы весов $W$ и $U$. Результаты этих преобразований комбинируются для получения значения запоминающего $\tr{c}{t}$ и скрытого состояния $\tr{h}{t}$.


\begin{figure*}[h]
	\centering
	\begin{subfigure}[c]{0.5\textwidth}
		\input{tables/lstm.tikz}
	\end{subfigure}	
	\begin{subfigure}[c]{0.45\textwidth}
		\begin{align}
	f_t = \sigma (\tr{x}{t} &\otimes U_f + \tr{h}{t-1} \otimes W_f)\\
	\tilde c_t = tanh (\tr{x}{t} &\otimes U_c + \tr{h}{t-1} \otimes W_c)\\
	i_t = \sigma (\tr{x}{t} &\otimes U_i + \tr{h}{t-1} \otimes W_i)\\
	o_t = \sigma (\tr{x}{t} &\otimes U_o + \tr{h}{t-1} \otimes W_o)\\
	c_t = f_t &\otimes c_{t-1} + i_t \otimes \tilde c_t \\
	h_t = o_t &\otimes tanh(c_t)
	\end{align}
	\end{subfigure}
	\caption{{\bfseries LSTM слой.} \\*
	$\otimes$ - поэлементное умножение, $+$ - поэлементное сложение. }
	\label{fig:lstm}
\end{figure*}

\subsubsection{Архитектура нейронных сетей}
Все нейронные сети были реализованы с помощью библиотек Keras\cite{chollet_keras_2015} , TensorFlow.

\input{tables/all_schemes}

{\bfseries Полносвязные модели} (DNN model 1 - рисунок \ref{fig:dnn_1_scheme}). Первая простейшая полносвязная модель состоит из входного слоя, принимающего нуклеотидный контекст, слоя, уплощающего данные в один вектор, четырех полносвязных нейронов с функцией активации softmax, выходом которых являются вероятности для четырех выходных букв. Число параметров модели $16n + 4$, где $n$ -- размер контекста.

(DNN model 2 -- рисунок \ref{fig:dnn_2_scheme}). В полносвязную модель добавлен слой нейронов с активацией ReLU. Число параметров модели $16n + 24$, где $n$ -- размер контекста.


{\bfseries Сверточные модели} (CNN model 1 - рисунок \ref{fig:cnn_schemes}).
Простейшая сверточная модель состоит из слоя сверточных нейронов (с функцией активации relu), который работает непосредственно с матрицей контекста, далее выход свертки уплощается в вектор и подается в слой из 4 решающих выходных полносвязных нейронов (с функцией активации softmax). Конфигурация сверточного слоя может быть различной. Мы исследовали комбинации из разного числа нейронов, с разным размером фильтра (kernel), которые обрабатывают контекст с различным шагом (stride, по умолчанию шаг сверточного фильтра равен 0).

Варьировали параметры сверточного слоя: 
\begin{enumerate}[noitemsep,topsep=0pt]
		\item размер сверточного фильтра $n$, что обозначено как k$n$,
		\item количество сверточных фильтров $m$, что обозначено как k$n$*$m$,
		\item шаг применения сверточнго фильтра (stride). 
			\end{enumerate}
 
 
{\bfseries Рекуррентыне модели } (RNN model 1 - рисунок \ref{fig:rnn_scheme}) Рекуррентная модель состояла из одного LSTM (long short-term memory) слоя, который содержал разное число скрытых состояний, и выходного слоя полносвязных нейронов с функцией активации softmax. 

{\bfseries Модели для двухстороннего контекста} отдельно обрабатывали контекста с каждой стороны, выход конкатенировался и подавался в решающий слой.

Для сверточных моделей с двухсторонним контекстом: каждый контекст обрабатывался в сверточном слое с указанными параметрами, выходные векторы конкатенировались и подавались в решающий слой.


\subsubsection{Deep Image Prior}
Deep Image Prior --  нейронная сеть с архитектурой автоэнкодера с пробросочными соединениями, подробно описана в \cite{ulyanov_deep_2018}

Данная архитектура была адаптирована для геномной последовательности, закодированной в виде one-hot-encoded матрицы соответственного размера $n\times 4$, где $n$ – длина обрабатываемой геномной области.
В данной архитектуре двумерные функции свертки, пулинга, нормализации, апсемплинга были заменены на соответствующие одномерные аналоги.
Суть подхода закллючается в следующем:

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Выбиралась геномная область длиной 500,000 нуклеотидов. Такой размер области позволял наиболее эффективно проводить расчеты.
	\item На области случайно равномерно выбиралось  10\% нуклеотидов, которые далее предсказывались, которые обозначаются как тест или маска.
	\item Нейронная сеть обучалась получать из случайно сгенерированного массива чисел целевую геномную последовательность. Функция потерь при этом не учитывала тестовые (маскированные) нуклеотиды.
	\item Когда функция потерь переставала уменьшаться, обучение останавливалось. Проверялось, что же предсказывает модель на месте замаскированных нуклеотидов.
\end{enumerate} 



