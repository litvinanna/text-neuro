\section{Методы}
\subsection{Данные}
Все нуклеотидные последовательности были закодированны в one-hot-encoded векторы (единичные нуклеотиды) и матрицы (последовательности), где каждому нуклеотиду соответствует один из четырехмерных векторов (0, 0, 0, 1), (0, 0, 1, 0), (0, 1, 0, 0), (1, 0, 0, 0). Это позволяет добиться независимого влияния нуклеотидов на предсказание и используется в категориальных предсказаниях.
\subsubsection{Построение выборки контекстов}

Последовательности выбирались из генома  Escherichia coli (Escherichia coli str. K-12 substr. MG1655, сборка GCF 000005845.2).

Выборка представляет собой набор контекстов (предикторных областей) определенной длины и соответственных нуклеотидов для предсказания.

Выборка состояла из нескольких частей --  для обучения алгоритма (train), валидации в процессе обучения (validate), тестирования (test). При этом эти части были взяты из непересекающихся геномных областей, чтобы предотвратить выучивание алгоритмом последовательности нуклеотидов.

Для статистической проверки каждого метода было построено 30 выборок. Во всех 30 выборках области, соответствующие тренировочной и валидационной части, были разные.

Размер тренировочной выборки обычно составлял 100,000 нуклеотидов, валидационной и тренирочной -- по одной десятой, соответственно 10,000 и 10,000.

Предикторные области (контексты) для каждого нуклеотида находились с 5' конца нуклеотида, их размер варьировал -- 3, 6, 12, 24 нуклеотида. Также создавались выборки со предикторной областью, сдвинутой относительно предсказываемого нуклеотида в 5' сторону на 1, 2, 3, 6, 12, 50 нуклеотидов.

В качестве предикторов также использовались контексты с двух сторон от нуклеотида, причем каждый

Все предикторные обасти и предсказываемые нуклеотиды были закодированы в виде one-hot-encoded векторов и матриц.

\subsection{Нейронные сети}

В качестве функции активации использовалась функция softmax и ReLU (уравнение \ref{eq:softmax}).
\begin{align} \label{eq:softmax}
\text{softmax}(X) &= exp(X - \max_{axis}(X)) \\
\text{ReLU}(X) &= \max_{x_i \in X}(x_i, 0)
\end{align}


В нашем случае функцией потерь служила категориальная кроссэнтропия
\begin{equation} \label{eq:cross}
\text{categorical\_crossentropy}(Y_{pred}, Y_{true}) = -\sum_{x}{Y_{pred}(X)\log(Y_{true}(X))}
\end{equation}

Во всех архитектурах использовался оптимизатор  Adam \cite{kingma_adam:_2014} с параметрами по умолчанию. 
Во время обучения контролировалась точность предсказания на валидационной выборке, с прекращением роста точности обучение останавливалось.

Для статистического сравнения точности сетей на наборе датасетов использовался критерий Манна-Уитни.

В архитектурах нейронный сетей использовалось несколько типов слоев.
\paragraph{Полносвязный слой}
Каждый из k нейронов в полносвязном слое вычисляет сумму результатов поэлементного произведения вектора входных значений на вектор весов, что можно записать как:
\begin{equation}
	\text{fully_connected}(X)_{k} = X_{1\times L} \cdot W^k_{L\times 1} = x_1 w^k_1 +  \dots x_L w^k_L 
\end{equation}
где $L$ -- длина входного вектора, $X_{1\times L}$ -- входной вектор, $W^k_{L\times 1}$ -- вектор весов.

\paragraph{Сверточный слой}

Для работы с последовательностями была использована одномерная свертка. Слой состоит из нескольких нейронов, каждый со своей весовой матрицей. Каждый нейрон сканирует последовательность вдоль. Под размером нейрона мы понимаем длину скользящего окна. Формально, вычисляется следующее:
\begin{equation}
	\text{convolution}(X)_{ik} = \sum_{m=0}^{M-1}\sum_{n=0}^{N-1} W_{mn}^k X_{i+m, n}
\end{equation}
где $X$ -- это вход, $i$ -- индекс выходной позиции, $k$ -- номер нейрона в слое. Каждое сверточное ядро это матрица $W^k$ размером $M \times N$, где $M$ размер окна, $N$ число входных каналов (в случае первого слоя это 4, для слоев более высокого порядка число каналов определяется число нейронов в предыдущем слое).
\paragraph{Рекуррентные слои} В наших архитектурах мы использовали одну вариацию рекуррентного слоя -- слой долгосрочной памяти (LSTM, long short-term memory). 
  
\subsubsection{Архитектура нейронных сетей}
Все нейронные сети были реализованы с помощью библиотек Keras\cite{chollet_keras_2015} , TensorFlow.

\input{tables/all_schemes}

{\bfseries Полносвязные модели} (DNN model 1 - рисунок \ref{fig:dnn_1_scheme}). Первая простейшая полносвязная модель состоит из входного слоя, принимающего нуклеотидный контекст, слоя, уплощающего данные в один вектор, четырех полносвязных нейронов с функцией активации softmax, выходом которых являются вероятности для четырех выходных букв. Число параметров модели $16n + 4$, где $n$ -- размер контекста.

(DNN model 2 -- рисунок \ref{fig:dnn_2_scheme}). В полносвязную модель добавлен слой нейронов с активацией ReLU. Число параметров модели $16n + 24$, где $n$ -- размер контекста.


{\bfseries Сверточные модели} (CNN model 1 - рисунок \ref{fig:cnn_schemes}).
Простейшая сверточная модель состоит из слоя сверточных нейронов (с функцией активации relu), который работает непосредственно с матрицей контекста, далее выход свертки уплощается в вектор и подается в слой из 4 решающих выходных полносвязных нейронов (с функцией активации softmax). Конфигурация сверточного слоя может быть различной. Мы исследовали комбинации из разного числа нейронов, с разным размером фильтра (kernel), которые обрабатывают контекст с различным шагом (stride, по умолчанию шаг сверточного фильтра равен 0).

Были использованы следующие конфигурации сверточного слоя: \begin{enumerate}
		\item (kernel = 3)*3, три сверточных фильтра размером 3
		\item (kernel = 6)*3, три сверточных фильтра размером 6
		\item (kernel = 3)*3 stride = 3, три сверточных фильтра размером 3, которые обрабатывают матрицу с шагом 3.
	\end{enumerate}
 
{\bfseries Рекуррентыне модели } (RNN model 1 - рисунок \ref{fig:rnn_scheme}) Рекуррентная модель состояла из одного LSTM (long short-term memory) слоя, который содержал разное число скрытых состояний, и выходного слоя полносвязных нейронов с функцией активации softmax. 



\subsection{Deep Image Prior}
Deep Image Prior --  нейронная сеть с архитектурой автоэнкодера с пробросочными соединениями, подробно описана в \cite{ulyanov_deep_2018}

Данная архитектура была адаптирована для геномной последовательности, закодированной в виде one-hot-encoded матрицы соответственного размера $n\times 4$, где $n$ – длина обрабатываемой геномной области.
В данной архитектуре двумерные функции свертки, пулинга, нормализации, апсемплинга были заменены на соответствующие одномерные аналоги.
Суть подхода закллючается в следующем:

\begin{enumerate}
	\item Выбиралась геномная область длиной 500,000 нуклеотидов. Такой размер области позволял наиболее эффективно проводить расчеты.
	\item На области случайно равномерно выбиралось  10\% нуклеотидов, которые далее предсказывались, которые обозначаются как тест или маска.
	\item Нейронная сеть обучалась получать из случайно сгенерированного массива чисел целевую геномную последовательность. Функция потерь при этом не учитывала тестовые (маскированные) нуклеотиды.
	\item Когда функция потерь достигала низких значений, обучение останавливалось. Проверялось, что же предсказывает модель на месте замаскированных нуклеотидов.
\end{enumerate} 



